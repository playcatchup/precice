#!/usr/bin/python

import json
import csv
import sys
import datetime
import argparse
import os


def requireModules(modules):
    import importlib
    notFound = [m for m in modules if importlib.util.find_spec(m) is None]
    if (notFound):
        def concat(l):
            if len(l) == 1:
                return l[0]
            else:
                return ", ".join(l[:-1]) + f", and {l[-1]}"
        print(
            f"This command requires the following additional dependencies: {concat(modules)}")
        print(f"The following is/are missing: {concat(notFound)}")
        sys.exit(1)


def loadRobust(content):
    try:
        return json.loads(content)
    except json.decoder.JSONDecodeError:
        print("Damaged input detected")
        content += "]}"
        return json.loads(content)


def readRobust(filename):
    with open(filename, 'r') as openfile:
        return loadRobust(openfile.read())


def printWide(df):
    import pandas
    with pandas.option_context('display.width', None,
                               'display.max_rows', None,
                               'display.max_columns', None,
                               'display.max_colwidth', None):
        print(df)


def ns_to_unit(obj, unit):
    import pandas
    if isinstance(obj, (pandas.Timedelta, pandas.Timestamp)):
        return ns_to_unit(obj.value, unit)

    return obj * {
        'ns': 1,
        'us': 1e-3,
        'ms': 1e-6,
        's': 1e-9,
        'm': 1e-9/60,
        'h': 1e-9/3600
    }[unit]


def alignEvents(events):
    """Alignes passed events of multiple ranks and or participants.
    All ranks of a participant align at initialization, ensured by a barrier in preCICE.
    Primary ranks of all participants align after successfully establishing primary connections.
    """
    assert "events" in events and "eventDict" in events, "Not a loaded event file"
    assert len(events.get("events")) > 0, "No participants in the file"
    participants = events["events"].keys()
    grouped = events["events"]

    # Align ranks of each participant
    for participant, ranks in grouped.items():
        if len(ranks) == 1:
            continue
        print(f"Aligning {len(ranks)} ranks of {participant}")
        intraSyncID = [id for name, id in events["eventDict"].items(
        ) if name == "com.initializeIntraCom"][0]
        syncs = {rank: e["ts"]+e["dur"]
                 for rank, data in ranks.items()
                 for e in data["events"]
                 if e["eid"] == intraSyncID
                 }

        firstSync = min(syncs.values())
        shifts = {rank: firstSync-tp for rank, tp in syncs.items()}
        print(shifts)

        for rank, data in ranks.items():
            for e in data["events"]:
                e["ts"] += shifts[rank]

    if len(grouped) == 1:
        return events

    # Align participants
    primaries = [name for name, ranks in events["events"].items()
                 if 0 in ranks]

    for lonely in set(events["events"].keys()).difference(primaries):
        print(f"Cannot align {lonely} as event file of rank 0 is missing.")

    # Cannot align anything
    if len(primaries) == 1:
        return events

    # Find synchronization points
    syncEvents = ["m2n.acceptPrimaryRankConnection.",
                  "m2n.requestPrimaryRankConnection."]
    # Event ID -> Remote name
    syncIDs = {id: name.rsplit(".", 1)[1]
               for id, name in events["eventDict"].items()
               if any([check in name for check in syncEvents])
               }
    syncs = {
        local: {
            remote: e["ts"] + e["dur"]
            for e in events["events"][local][0]["events"]
            for eid, remote in syncIDs.items()
            if eid == e["eid"]
        }
        for local in primaries
    }

    def hasSync(l, r): return syncs.get(l) and syncs.get(
        l).get(r) and syncs.get(r) and syncs.get(r).get(l)
    shifts = {
        (local, remote): syncs[local][remote] - syncs[remote][local]

        # all unique participant combinations
        for local in primaries
        for remote in primaries
        if local < remote
        if hasSync(local, remote)
    }
    for (local, remote), shift in shifts.items():
        print(f"Aligning {remote} ({shift}us) with {local}")
        for rank, data in events["events"][remote].items():
            data["meta"]["unix_us"] += shift
            for e in data["events"]:
                e["ts"] += shift

    return events


def groupEvents(events, initTime, nameIDOffset):
    completed = []
    active = {}  # name to event data

    for event in events:
        type = event["et"]
        if type == "n":
            continue

        name = event["eid"]
        # Handle event starts
        if type == "b":
            # assert(name not in active.keys())
            if (name in active.keys()):
                print(f"Ignoring start of active event {name}")
            else:
                event["ts"] = int(event["ts"])
                active[name] = event
        # Handle event stops
        elif type == "e":
            # assert(name in active.keys())
            if (name not in active.keys()):
                print(f"Ignoring end of inactive event {name}")
            else:
                begin = active[name]
                active.pop(name)
                begin["dur"] = int(event["ts"]) - begin["ts"]
                begin["ts"] = int(begin["ts"]) + initTime
                begin.pop("et")
                completed.append(begin)
        # Handle event data
        elif type == "d":
            if (name not in active.keys()):
                print(f"!! {name} not yet active")
            else:
                d = active[name].get("data", {})
                d[event["dn"]] = int(event["dv"])
                active[name]["data"] = d

    # Handle leftover events in case of a truncated input file
    if active:
        lastTS = min(map(lambda e: e["ts"] + e["dur"], completed))
        for event in active.values():
            name = event["eid"]
            print(f"Truncating event without end {name}")
            begin = active[name]
            begin["ts"] = int(begin["ts"]) + initTime
            begin["dur"] = lastTS - begin["ts"]
            begin.pop("et")
            completed.append(begin)

    return [
        e | {"eid": e["eid"]+nameIDOffset}
        for e in sorted(completed, key=lambda e: e["ts"])
    ]


def loadEventFile(filename, nameIDs):
    print(f"Loading events from {filename}")
    content = open(filename, "r").read()
    json = loadRobust(content)

    unix_us = int(json["meta"]["unix_us"])

    nameIDOffset = len(nameIDs)

    loadedNames = {int(e["id"]) + nameIDOffset: e["n"]
                   for e in json["events"]
                   if e["et"] == "n"
                   }

    allNames = loadedNames | nameIDs

    instanceData = {
        "meta": {
            "name": json["meta"]["name"],
            "rank": int(json["meta"]["rank"]),
            "size": int(json["meta"]["size"]),
            "unix_us": unix_us,
            "tinit": json["meta"]["tinit"]
        },
        "events": groupEvents(json["events"], unix_us, nameIDOffset)
    }

    return (instanceData, allNames)


def mergeDuplicateNames(contents, eventIDs):
    # eventIDs is id -> name
    uniqueEvents = {
        name: id
        for id, name in enumerate(set(eventIDs.values()))
    }
    idMap = {
        oldid: uniqueEvents[name]
        for oldid, name in eventIDs.items()
    }

    for rank in contents:
        rank["events"] = [
            e | {"eid": idMap[e["eid"]]}
            for e in rank["events"]
        ]

    nameLookup = {id: name for name, id in uniqueEvents.items()}
    assert (len(nameLookup) == len(uniqueEvents)), "Some events went missing"

    return (contents, nameLookup)


def loadEventFiles(filenames):
    eventIDs = {}

    contents = []
    for fn in filenames:
        idata, eventIDs = loadEventFile(fn, eventIDs)
        contents.append(idata)

    contents, eventIDs = mergeDuplicateNames(contents, eventIDs)

    pnames = set([data["meta"]["name"] for data in contents])
    return {
        "eventDict": eventIDs,
        "events": {
            pname: {
                data["meta"]["rank"]: data
                for data in contents
                if data["meta"]["name"] == pname
            }
            for pname in pnames
        }
    }


class RankData:
    def __init__(self, data):
        meta = data["meta"]
        self.name = meta["name"]
        self.rank = meta["rank"]
        self.size = meta["size"]
        self.unix_us = meta["unix_us"]
        self.tinit = meta["tinit"]

        self.events = data["events"]

    @property
    def type(self):
        return "Primary" if self.rank == 0 else "Secondary"

    def toDataFrame(self, eventLookup):
        import pandas
        df = pandas.DataFrame(self.events)
        if "data" in df:
            df.drop("data", axis=1, inplace=True)
        df["dur"] = pandas.to_timedelta(df["dur"], unit='microseconds')
        df["ts"] = pandas.to_datetime(df["ts"], unit="us", origin="unix")
        df.insert(0, 'participant', self.name)
        df.insert(1, 'rank', self.rank)
        df["eid"] = df["eid"].apply(lambda id: eventLookup[id])
        return df


class Run:
    def __init__(self, filename):
        print(f"Reading events file {filename}")
        import json
        with open(filename, 'r') as f:
            content = json.load(f)

        self.eventLookup = {int(id): name for id,
                            name in content["eventDict"].items()}
        self.data = content["events"]

    def iterRanks(self):
        for pranks in self.data.values():
            for d in pranks.values():
                yield RankData(d)

    def iterParticipant(self, name):
        for d in self.data[name].values():
            yield RankData(d)

    def participants(self):
        return self.data.keys()

    def lookupEvent(self, id):
        return self.eventLookup[id]

    def toTrace(self):
        pids = {name: id for id, name in enumerate(self.participants())}
        metaEvents = [
            {"name": "process_name", "ph": "M", "pid": pid, "args": {"name": name}}
            for name, pid in pids.items()
        ] + [
            {"name": "thread_name", "ph": "M", "pid": pid,
                "tid": rank.rank, "args": {"name": rank.type}}
            for name, pid in pids.items()
            for rank in self.iterParticipant(name)
        ]

        mainEvents = []
        for rank in self.iterRanks():
            pid, tid = pids[rank.name], rank.rank
            for e in rank.events:
                en = self.lookupEvent(e["eid"])
                mainEvents.append({
                    "name": en,
                    "cat": "Solver" if en.startswith("solver") else "preCICE",
                    "ph": "X",  # complete event
                    "pid": pid,
                    "tid": tid,
                    "ts": e["ts"],
                    "dur": e["dur"]
                } | {} if "data" not in e else {"args": e["data"]})

        return {"traceEvents": metaEvents + mainEvents}

    def toExportList(self, unit=None):
        return [
            {
                "participant": rank.name,
                "rank": rank.rank,
                "size": rank.size,
                "event": self.lookupEvent(e["eid"]),
                "timestamp": e["ts"],
                "duration": ns_to_unit(e["dur"]*1e3, unit) if unit else e["dur"],
                "data": "" if "data" not in e else str(e["data"])
            }
            for rank in self.iterRanks()
            for e in rank.events
        ]

    def toDataFrame(self):
        import pandas
        return pandas.concat([
            rank.toDataFrame(self.eventLookup)
            for rank in self.iterRanks()
        ])


def traceCommand(eventfile, outfile):
    run = Run(eventfile)
    traces = run.toTrace()
    json_object = json.dumps(traces, indent=2)
    print(f"Writing to {outfile}")
    with open(outfile, "w") as outfile:
        outfile.write(json_object)
    return 0


def exportCommand(eventfile, outfile, unit):
    run = Run(eventfile)
    total = run.toExportList(unit)
    fieldnames = total[0].keys()
    print(f"Writing to {outfile}")
    with open(outfile, 'w', newline='') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(total)
    return 0


def plotCommand(events, eventfile, unit):
    requireModules(["pandas", "matplotlib"])
    import pandas
    import matplotlib.pyplot as plt
    import matplotlib

    def to_unit(o): return ns_to_unit(o, unit)

    run = Run(eventfile)
    total = run.toDataFrame()

    allRanks = list(total[["participant", "rank"]
                          ].drop_duplicates().itertuples(index=False))
    colorMap = {pr: c for pr, c in zip(
        allRanks, matplotlib.cm.tab20(range(len(allRanks))))}

    nevents = len(events)
    fig, axs = plt.subplots(nrows=nevents, sharex=True)
    for event, ax in zip(events, axs if nevents > 1 else [axs]):
        ax.set_title(event)
        ax.margins(y=0.2)
        edf = total[total["eid"] == event]

        yticks = []

        for i, (name, group) in enumerate(edf.groupby(["participant", "rank"])):
            ax.hlines([str(i)]*len(group.ts),
                      xmin=group.ts.apply(to_unit),
                      xmax=(group.ts+group.dur).apply(to_unit),
                      colors=colorMap[name],
                      linewidth=2
                      )
            yticks.append(f"{name[0]}:{name[1]}")

        ax.yaxis.set_major_locator(
            matplotlib.ticker.FixedLocator(ax.get_yticks()))
        ax.grid(axis="y", color='0.9')
        ax.yaxis.tick_right()
        ax.set_yticklabels(yticks)

    plt.tight_layout()
    plt.show()

    return 0


def analyzeCommand(eventfile, participant, outfile=None, unit='us'):
    requireModules(["pandas"])
    import pandas

    run = Run(eventfile)
    df = run.toDataFrame()

    # Filter by participant
    df = df[df["participant"] == participant]
    df.drop("participant", axis=1, inplace=True)

    # Convert duration to requested unit
    df.dur = df.dur.apply(lambda td: ns_to_unit(td, unit))

    all = df.groupby("eid").agg(
        {"dur": [("min", "min"), ("mean", "mean"), ("max", "max"), ("sum", "sum"), ("count", "count")]})
    all.columns = all.columns.droplevel()

    isParallel = len(df["rank"].unique()) > 1
    if isParallel:
        primary = df[df["rank"] == 0].groupby("eid").agg(
            {"dur": [("Pmin", "min"), ("Pmean", "mean"), ("Pmax", "max"), ("Psum", "sum"), ("Pcount", "count")]})
        secondaries = df[df["rank"] > 0].groupby("eid").agg(
            {"dur": [("Smin", "min"), ("Smean", "mean"), ("Smax", "max"), ("Ssum", "sum"), ("Scount", "count")]})

        primary.columns = primary.columns.droplevel()
        secondaries.columns = secondaries.columns.droplevel()

        joined = pandas.concat([all, primary, secondaries], axis=1)
    else:
        joined = all

    joined.index.rename("event", inplace=True)
    printWide(joined)
    if (outfile):
        print(f"Writing to {outfile}")
        joined.to_csv(outfile)

    return 0


def mergeCommand(files, outfile, align):
    # Default to loading from the local directory
    if not files:
        files = ["."]

    resolved = []
    for path in files:
        if os.path.isfile(path):
            resolved.append(path)
        if os.path.isdir(path):
            import glob
            detected = glob.glob("precice-*-*-*.json", root_dir=path)
            print(f"Autodetected {len(detected)} files in {path}")
            resolved += [os.path.join(path, p) for p in detected]
        else:
            print(f"Not sure what do do with {path}")

    merged = loadEventFiles(resolved)

    if align:
        merged = alignEvents(merged)

    print(f"Writing to {outfile}")
    with open(outfile, 'w', newline='') as file:
        json.dump(merged, file, separators=(",", ":"))

    return 0


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="", epilog="")
    subparsers = parser.add_subparsers(title='commands',
                                       description='Note that some commands may require pandas or matplotlib',
                                       help='additional help',
                                       dest="cmd"
                                       )
    # parser.add_argument("-v", "--verbose", help="Print verbose output")
    parser.add_argument("-u", "--unit", choices=["h", "m", "s", "ms", "us"], default="us",
                        help="The duration unit to use")

    analyze_help = """Analyze events data of a given solver.
    Events durations are displayed in the unit of choice and grouped in three sections:
    For parallel solvers, aggregates over all ranks, only the Primary rank, and only the Secondary ranks.
    """
    analyze = subparsers.add_parser('analyze',
                                    help=analyze_help.splitlines()[0], description=analyze_help)
    analyze.add_argument("participant", type=str,
                         help="The participant to analyze")
    analyze.add_argument("eventfile", nargs="?", type=str, default="events.json",
                         help="The event file to process")
    analyze.add_argument("-o", "--output",
                         help="Write the result to CSV file")

    trace_help = "Transform events to the Trace Event Format."
    trace = subparsers.add_parser('trace',
                                  help=trace_help.splitlines()[0], description=trace_help)
    trace.add_argument("eventfile", type=str, nargs="?", default="events.json",
                       help="The event file to process")
    trace.add_argument("-o", "--output", default="trace.json",
                       help="The resulting trace file")

    export_help = "Exports the events as a CSV file."
    export = subparsers.add_parser('export',
                                   help=export_help.splitlines()[0], description=export_help)
    export.add_argument("eventfile", nargs="?", type=str, default="events.json",
                        help="The event files to process")
    export.add_argument("-o", "--output", type=str, default="events.csv",
                        help="The CSV file to export to.")

    plot_help = "Plot given events of the recorded events."
    plot = subparsers.add_parser('plot',
                                 help=plot_help.splitlines()[0], description=plot_help)
    plot.add_argument("events", type=str,
                      help="The events to plot")
    plot.add_argument("eventfile", nargs="?", type=str, default="events.json",
                      help="The event file to process")

    merge_help = "Transform multiple events to the Trace Event Format."
    merge = subparsers.add_parser('merge',
                                  help=merge_help.splitlines()[0], description=merge_help)
    merge.add_argument("files", nargs="*",
                       help="The event files to process, or nothing to autodetect")
    merge.add_argument("-o", "--output", type=str, default="events.json",
                       help="The resulting event file.")
    merge.add_argument("-a", "--align", type=bool,
                       default=True, help="Align participants?")

    args = parser.parse_args()

    dispatcher = {
        "trace": lambda ns: traceCommand(ns.eventfile, ns.output),
        "export": lambda ns: exportCommand(ns.eventfile, ns.output, ns.unit),
        "analyze": lambda ns: analyzeCommand(ns.eventfile, ns.participant, ns.output,  ns.unit),
        "plot": lambda ns: plotCommand(ns.events.split(","), ns.eventfile,  ns.unit),
        "merge": lambda ns: mergeCommand(ns.files, ns.output, ns.align)
    }

    def showHelp(ns):
        parser.print_help()
        return 1

    sys.exit(dispatcher.get(args.cmd, showHelp)(args))
